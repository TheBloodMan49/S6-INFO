{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "302fe43c",
   "metadata": {},
   "source": [
    "# Le Boosting: un méta algorithme d'apprentissage\n",
    "\n",
    "un méta-algorithme d'aprentissage est un algorithme permettant de combiner un ensemble d'instances d'un classifieur particulier. Le boosting est un de ces méta-algorithme:\n",
    "- son principe est de combiner un ensemble de **classifieurs faibles** (classifiers en sous apprentissage avec performance supérieur au hasard)\n",
    "- la construction des classifieurs qui sont combinés est itérative et dépend de la performance des précédents (chaque classifieur ajouté dans la combinaison tente d'améliorer l'ensemble précédent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ad64b9",
   "metadata": {},
   "source": [
    "[AdaBoost](https://fr.wikipedia.org/wiki/AdaBoost) (ou adaptive boosting) est un méta-algorithme de boosting introduit par [Yoav Freund](https://fr.wikipedia.org/wiki/Yoav_Freund) et [Robert Schapire](https://fr.wikipedia.org/wiki/Robert_Schapire) en 19971. AdaBoost fonctionne pour les problèmes de classification binaires (2 classes) et améliore les performances de n'importe quel algorithme d'apprentissage. L'algorithme simplifié est le suivant:\n",
    "1) soit un ensemble d'observations (exemples du corpus d'aprentissage), nous allons associer à chaque exemple un **poids** qui sera le même pour tous les exemples au début de l'algorithme\n",
    "2) puis nous allons apprendre un classifieur faible (par exemple un arbre de décision à deux feuilles seulement) à partir de ces données\n",
    "3) ce classifieur en sous-apprentissage va inévitablement commettre beaucoup d'erreurs sur ces propres données d'apprentissage: les exemples que le classifieur ne sait classer, nous allons les ***booster***, nous allons incrémenter leur **poids**.\n",
    "4) nous allons alors apprendre un nouveau classifieur faible sur les mêmes données mais où certains exemples (ceux mal classés par le classieur précédent) comptent plus et le nouveau classifieur va s'attacher à bien classer en priorité ceux-là, il viendra donc corriger les erreurs du précédent.\n",
    "5) boucle sur point **3** afin de générer un ensemble de classifieur\n",
    "6) le classifieur de boosting final correspond à un vote de l'ensemble de ces classifieurs construits et permet de générer à partir d'un ensemble de classifieur faible un classifieur de boosting très fort.\n",
    "    - chaque classifieur va voter pour pos avec 1 et neg avec -1\n",
    "    - on va sommer le vote de tous les classifieur, si la somme est supérieure à zéro le boosting prédira pos sinon neg\n",
    "    - le boosting n'est pas républicain: chaque classifieur peut avoir une voix au chapitre différente en fonction de sa performance individuelle son vote -1 ou +1 sera pondéré par un coefficient $\\lambda$\n",
    "\n",
    "---\n",
    "Nous allons maintenant découvrir l'algorithme précisemment mais étape par étape au cours de ce TP:\n",
    "- nous allons tester cet algorithme sur le corpus de [imdb.com](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) où la tâche binaire de classification est de prédire si la critique du film est positive ou non\n",
    "- nous allons choisir un arbre de décision à deux feuilles, autement appelé en anglais *decision stumps*, donc un arbre qui fait un seul test binaire, ici le test binaire sera la présence d'un mot ou non dans la critique du film\n",
    "- l'algorithme de boosting demande que l'on prédise la valeur -1 ou +1 selon la classe: mettons -1 pour négatif et +1 pour positif\n",
    "\n",
    "## ÉTAPE 1: préparation des données\n",
    "\n",
    "\n",
    "### 1) chargement des données de imdb"
   ]
  },
  {
   "cell_type": "code",
   "id": "03eedf60",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "data_train = pd.read_json('imdb-trn.json')\n",
    "data_train.columns=['classe','critique']\n",
    "data_test = pd.read_json('imdb-tst.json')\n",
    "data_test.columns=['classe','critique']\n",
    "\n",
    "\n",
    "\n",
    "data_train=data_train.sample(frac=0.1) #le corpus d'aprentissage contient 25000 exemples, on peut réduire un peu si le code est trop lent\n",
    "\n",
    "data_train.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e81db812",
   "metadata": {},
   "source": [
    "### 2) nettoyage des données\n",
    "- pour assurer une meilleure généralisation nous allons faire un nettoyage minimal des donnés\n",
    "- on pourrait par exemple *tokenizer* et lémmatiser les mots proprements (par exemple avec [nltk](https://stackoverflow.com/questions/47557563/lemmatization-of-all-pandas-cells)) \n",
    "- ici ce n'est pas l'objectif du TP, on peut se contenter de la question suivante:\n",
    "- **QUESTION**: supprimer la ponctuation, les mots contenant des chiffres et passez tous les mots en bas de casse"
   ]
  },
  {
   "cell_type": "code",
   "id": "49f0e9c3",
   "metadata": {},
   "source": [
    "data_train['critique'] = data_train['critique'].str.replace('[^\\w\\s]','', regex=True).str.replace('\\w*\\d+\\w*','', regex=True).str.lower()\n",
    "\n",
    "data_train.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6ed36889",
   "metadata": {},
   "source": [
    "pour faciliter le traitement ultérieur (vérifier dans une critique si un mot est utilisé ou non par l'auteur) nous allons utiliser la structure de données suivante: \n",
    "1) **critique_train/test**: une liste de dictionnaire, chaque dictionnaire contiendra comme entrées les mots utilisés dans la critique correspondante. \n",
    "2) **classes_train/test**: la liste des classes (-1 ou -1) pour chaque critique"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9ca6c11",
   "metadata": {},
   "source": [
    "from collections import Counter\n",
    "critiques_train =  [dict(Counter(critique.split())) for critique in data_train['critique'].to_list()]\n",
    "critiques_test =  [dict(Counter(critique.split())) for critique in data_test['critique'].to_list()]\n",
    "print(data_train['critique'].iloc[0],' -> ',critiques_train[0])\n",
    "classes_train = [1 if opinion == 'pos' else -1 for opinion in data_train['classe'].to_list()]\n",
    "classes_test = [1 if opinion == 'pos' else -1 for opinion in data_test['classe'].to_list()]\n",
    "print(data_train['classe'].iloc[0:5],' -> ',classes_train[0:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c84a096",
   "metadata": {},
   "source": [
    "## ÉTAPE 2: le classifieur faible\n",
    "- ce classifieur sera une arbre de décision à 2 feuilles appelé *decision stump*\n",
    "- le principe de boosting étant de pondérer (booster) les exemples d'apprentissage ,il nous faut un vecteur de poids et que notre classifieur le prenne en compte pour calculer l'erreur qu'il commet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1273878",
   "metadata": {},
   "source": [
    "### 1) Initialisation des poids\n",
    "- le boosting commence en considérant chaque exemple d'apprentissage équivalent en leur attribuant un poids identique\n",
    "- l'ensemble des poids doit sommer à 1 pour se conformer à une distribution de probabilité"
   ]
  },
  {
   "cell_type": "code",
   "id": "b750905c",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "weigths = np.ones(len(critiques_train),'float')/len(critiques_train)\n",
    "weigths"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf654a09",
   "metadata": {},
   "source": [
    "### 2) structure du classifeur (*decision stump*)\n",
    "**QUESTION**: écrivez le code de la méthode ***fit(..)*** du classifieur dont la description est donnée ci-après, cette fonction permet pour un test binaire connu (un mot) de calculer la valeur des autres attributs de la classe"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4e1e85d",
   "metadata": {},
   "source": [
    "class decision_stump:\n",
    "    binary_rule=None                 # une string stockant le test binaire, ici un mot de la critique\n",
    "    yes_branch_prediction:float=None # prediction -1 ou +1 si le mot est présent dans la critique\n",
    "    no_branch_prediction:float =None # prediction -1 ou +1 si le mot est absent de la critique\n",
    "    global_error: float=None         #taux d'erreur global du classifieur sur le corpus d'apprentissage\n",
    "    def __init__(self,word):\n",
    "        self.binary_rule=word        #le constructeur prend en argument le mot qui servira de test binaire, la valeur des autres attributs seront estimés sur le corpus d'apprentissage\n",
    "       \n",
    "    \n",
    "    \"\"\"\n",
    "    fit(...) \n",
    "    Args:\n",
    "        list_of_samples: la liste des dictionnaires représentant les mots de chaque critique\n",
    "        list_of_classes: la liste des classes de chaque critique (-1 ou +1 représentant neg ou pos)\n",
    "        weigths        : liste de poids de boosting actuel associé à chaque example \n",
    "    do:\n",
    "        renseigne les champs: yes_branch_prediction, no_branch_prediction et global_error\n",
    "    Returns:\n",
    "        l'erreur globale du classifieur faible\n",
    "    \"\"\"\n",
    "    def fit(self,list_of_samples,list_of_classes,weigths):\n",
    "        yes_nb_pos=0\n",
    "        yes_nb_neg=0\n",
    "        no_nb_pos=0\n",
    "        no_nb_neg=0\n",
    "        for i in range(len(list_of_samples)):\n",
    "            if self.binary_rule in list_of_samples[i]:\n",
    "                if list_of_classes[i]==1:\n",
    "                    yes_nb_pos+=weigths[i]\n",
    "                else:\n",
    "                    yes_nb_neg+=weigths[i]  \n",
    "            else:\n",
    "                if list_of_classes[i]==1:\n",
    "                    no_nb_pos+=weigths[i]  \n",
    "                else:\n",
    "                    no_nb_neg+=weigths[i]\n",
    "        if yes_nb_pos>yes_nb_neg:\n",
    "            self.yes_branch_prediction=1 \n",
    "        else :\n",
    "            self.yes_branch_prediction=-1\n",
    "        if no_nb_pos>no_nb_neg:\n",
    "            self.no_branch_prediction=1 \n",
    "        else :\n",
    "            self.no_branch_prediction=-1\n",
    "            \n",
    "        self.global_error = yes_nb_neg + no_nb_pos\n",
    "        \n",
    "        return self.global_error\n",
    "    \n",
    "    \"\"\"\n",
    "    predict(...) \n",
    "    Args:\n",
    "        list_of_samples: la liste des dictionnaires représentant les mots de chaque critique        \n",
    "    do:\n",
    "        fait une prédiction -1 ou +1 pour chaque exemple\n",
    "    Returns:\n",
    "        une liste de prediction\n",
    "    \"\"\"\n",
    "    def predict(self,list_of_samples):        \n",
    "        if self.yes_branch_prediction is not None and self.no_branch_prediction is not None:\n",
    "            predictions=[]\n",
    "            for sample in list_of_samples:\n",
    "                if self.binary_rule in sample:\n",
    "                    predictions.append(self.yes_branch_prediction)\n",
    "                else:\n",
    "                    predictions.append(self.no_branch_prediction)\n",
    "            return predictions\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    #renvoi l'erreur du classifieur\n",
    "    def error(self): \n",
    "        return self.global_error \n",
    "    def __str__(self):\n",
    "        return f'exists the word \"{self.binary_rule}\" error={self.global_error}'\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "702db4df",
   "metadata": {},
   "source": [
    "estimons un classifieur arbitraire avec ***fit***:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5ef1c41",
   "metadata": {},
   "source": [
    "clf = decision_stump('great')\n",
    "error=clf.fit(critiques_train,classes_train,weigths)\n",
    "print('great=',error)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9694b6ec",
   "metadata": {},
   "source": [
    "### 3) faire de la prédiction\n",
    "- le classifieur produit doit pouvoir faire de la prédiction sur des données arbitraire une fois appris\n",
    "\n",
    "**QUESTION** complétez la méthode ***predict*** qui permet de faire de la prédiction\n",
    "\n",
    "testons la:"
   ]
  },
  {
   "cell_type": "code",
   "id": "cbd13add",
   "metadata": {},
   "source": [
    "prediction = clf.predict(critiques_train)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(1-accuracy_score(classes_train,prediction),'==',clf.error())\n",
    "print(classification_report(classes_train,prediction,digits=5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd6465f0",
   "metadata": {},
   "source": [
    "## ÉTAPE 3: l'algo de boosting\n",
    "- d'habitude on choisissait le meilleur arbre selon le critère de l'entropie\n",
    "- mais ici notre arbre complet a juste un nœud donc on peut tous les évaluer et choisir le meilleur de manière objective:\n",
    "- $ \\rightarrow $ celui qui minimise l'erreur sur le corpus d'apprentissage\n",
    "---\n",
    "### 1) liste des test binaires\n",
    "- **QUESTION**: dressez la liste (**allwords**) de tous les test binaires possibles (tous les mots)\n",
    "- **ATTENTION** la liste est longue, filtrer la pour ne conserver que les mots ayant été vus dans au moins X documents (ça s'appelle en anglais *cutoff*) prenez par exemple X=100 pour que votre futur code ne soit pas trop lent"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd658756",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "X = 100\n",
    "words_appearance_dict = {}\n",
    "for critique in critiques_train:\n",
    "    for word in critique.keys():\n",
    "        if word in words_appearance_dict:\n",
    "            words_appearance_dict[word]+=1\n",
    "        else:\n",
    "            words_appearance_dict[word]=1\n",
    "            \n",
    "allwords = [word for word in words_appearance_dict.keys() if words_appearance_dict[word]>X]\n",
    "print(len(allwords))\n",
    "print(allwords[0:100])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "75f7a997",
   "metadata": {},
   "source": [
    "### 2) structure de l'algo de boosting\n",
    "\n",
    "- voici la structure en classe de l'algo de boosting observez là et lisez les commentaires"
   ]
  },
  {
   "cell_type": "code",
   "id": "14f21838",
   "metadata": {},
   "source": [
    "import tqdm\n",
    "\n",
    "class Adaboost:\n",
    "    allwords=None         # liste des tests binaires possibles (liste de mots)\n",
    "    weigths=None          # vecteur de poids associés à chaque exemples\n",
    "    weak_learners = []    # liste des classifieurs faibles (décision stumps) appris à chaque itération de l'algo de boosting\n",
    "    poids_de_vote = []    # poids de vote de chacun des weak_learners dans la décision finale\n",
    "    n_iteration = 1       # nombre d'itération de l'algorithme: nombre de décision stumps à construire et à faire voter\n",
    "     \n",
    "\n",
    "    def __init__(self,allwords,n_iter=1): #constructeur qui prend en paramètre la liste des test binaires possibles et le nombre d'itération\n",
    "        self.allwords=allwords\n",
    "        self.n_iteration=n_iter\n",
    "    \"\"\"\n",
    "    round(...) \n",
    "    Args:\n",
    "        list_of_samples: la liste des dictionnaires représentant les mots de chaque critique\n",
    "        list_of_classes: la liste des classes de chaque critique (-1 ou +1 représentant neg ou pos)\n",
    "        weigths        : liste de poids de boosting actuel associé à chaque example\n",
    "        allwords       : liste de tous les mots possibles\n",
    "    do:\n",
    "        teste tous les classifieurs possibles et trouve le meilleur \n",
    "    Returns:\n",
    "        le meilleur classifieur\n",
    "    \"\"\"\n",
    "    def round(self,list_of_samples,list_of_classes):\n",
    "        bestclf = None\n",
    "        \n",
    "        for word in self.allwords:\n",
    "            clf = decision_stump(word)\n",
    "            error=clf.fit(list_of_samples,list_of_classes,self.weigths)\n",
    "            if bestclf is None or error<bestclf.error():\n",
    "                bestclf=clf\n",
    "                \n",
    "        return bestclf\n",
    "\n",
    "    \"\"\"\n",
    "    fit(...) \n",
    "    Args:\n",
    "        list_of_samples: la liste des dictionnaires représentant les mots de chaque critique\n",
    "        list_of_classes: la liste des classes de chaque critique (-1 ou +1 représentant neg ou pos)            \n",
    "    do:\n",
    "        apprends les n_iteration decision stumps\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def fit(self,list_of_samples,list_of_classes):\n",
    "        self.weigths = np.ones(len(list_of_samples),'float64')/len(list_of_samples)\n",
    "\n",
    "        for i in tqdm.tqdm(range(self.n_iteration)):\n",
    "            self.weak_learners.append(self.round(list_of_samples,list_of_classes))\n",
    "            \n",
    "            e = self.weak_learners[-1].error()\n",
    "            \n",
    "            alpha = 0.5*np.log((1-e)/e)\n",
    "            self.poids_de_vote.append(alpha)\n",
    "            Z = 2*np.sqrt(e*(1-e))\n",
    "            \n",
    "            for j in range(len(list_of_samples)):\n",
    "                y = list_of_classes[j]\n",
    "                h = self.weak_learners[-1].predict([list_of_samples[j]])[0]\n",
    "                \n",
    "                self.weigths[j]=self.weigths[j]*np.exp(-alpha*y*h)/Z\n",
    "       \n",
    "\n",
    "        \"\"\"\n",
    "        predict(...) \n",
    "        Args:\n",
    "            list_of_samples: la liste des dictionnaires représentant les mots de chaque critique\n",
    "            n              : number of weak classifier to use for prediction, if None, use all\n",
    "        do:\n",
    "            prédit la classe -1 ou 1 de chaque exemple\n",
    "        Returns:\n",
    "            liste de prédictions\n",
    "        \"\"\"\n",
    "    def predict(self,list_of_samples,n=None):       \n",
    "        if n is None:\n",
    "            n=len(self.weak_learners)\n",
    "            \n",
    "        predictions = np.zeros(len(list_of_samples))\n",
    "        for i in range(n):\n",
    "            predictions+=np.array(self.weak_learners[i].predict(list_of_samples))*self.poids_de_vote[i]\n",
    "        \n",
    "        predictions = np.sign(predictions)\n",
    "        return predictions\n",
    "                       \n",
    "        \n",
    "        \n",
    "    def __str__(self):\n",
    "        chaine=\"n_iter=\"+str(self.n_iteration)+\"\\n\"\n",
    "        for wl in self.weak_learners:\n",
    "            chaine+=str(wl)+'\\n'\n",
    "        return chaine\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c815aa84",
   "metadata": {},
   "source": [
    "- **QUESTION**: écrivez le code de la fonction ***round()*** qui teste l'ensemble des *decision stump* (pour chaque mot possible) et renvoi le meilleur (celui dont l'erreur globale est la plus faible)\n",
    "\n",
    "**ASTUCE**: penser à tqdm pour afficher une barre de progression, si le temps estimé est trop long, augmentez le *cutoff* précédent\n",
    "\n",
    "Maintenant testons votre fonction: le meilleur classifieur à l'iter 1 est...:"
   ]
  },
  {
   "cell_type": "code",
   "id": "c05b23e8",
   "metadata": {},
   "source": [
    "adatest = Adaboost(allwords,1)\n",
    "adatest.fit(critiques_train,classes_train)\n",
    "print(adatest)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1ef232d6",
   "metadata": {},
   "source": [
    "### 4) organisons la boucle de boosting\n",
    "- la fonction ***fit*** va implémenter la boucle principale de l'algorithme  \n",
    "    1) entrainer un classifieur faible (decision stumps) avec la dictribution de poids actuelle\n",
    "    2) nous devons calculer le poids du vote ($\\alpha$) du classifieur courant dans la combinaison finale\n",
    "    3) nous devons modifier les poids de chacun des exemples d'apprentissages en fonction des erreurs commises par le decision stump précédent \n",
    "    4) reboucler sur 1 tant que le nombre de classifieur faible entrainés est < à n_iteration\n",
    "    \n",
    "**QUESTION** complétez la méthode ***fit*** de la classe booting\n",
    "1) calculer le poids de vote ($\\alpha$)\n",
    "    - $\\alpha = \\frac{1}{2}ln(\\frac{1-\\epsilon}{\\epsilon})$ ou $\\epsilon$ est l'erreur commise par le classifieur\n",
    "    - mettez à jour ***poids_de_vote***\n",
    "\n",
    "2) mettez à jour les poids de ***weights***\n",
    "    - si weights = W\n",
    "    - $W_{t+1}(i)=\\frac{W_t(i)e^{-\\alpha_t*y_i*h_t(x_i)}}{Z_t}$\n",
    "    - $Z_t$ est le coeff de normalisation $Z_t=2\\sqrt{\\epsilon_t(1-\\epsilon_t)}$\n",
    "    - où $y_i$ est la vrai classe (-1 ou +1) de l'example et $h_t(x_i)$ est la prédiction du classifieur (decision stump courant)"
   ]
  },
  {
   "cell_type": "code",
   "id": "41ecbe23",
   "metadata": {},
   "source": [
    "ada = Adaboost(allwords,50)\n",
    "ada.fit(critiques_train,classes_train)\n",
    "print(ada)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "09db7709",
   "metadata": {},
   "source": [
    "### 5) faire de la prediction\n",
    "1) chaque exemple pour lequel nous voulons prédire sa classe devra passer dans chaque classifier faible qui fera une prédiction de -1 ou 1 pondérée par le poids de vote $\\alpha$\n",
    "2) nous ferons la somme de tous ces votes:\n",
    "    - si cette somme est >= 0 nous prédisons 1, sinon -1\n",
    "3) afin d'évaluer la qualité du boosting en fonction du nombre de decision stumps, nous ajoutons un paramètre ***n*** qui permettra de prédire qu'en utilisant les n premiers classifieurs sans avoir à réapprendre\n",
    "    \n",
    "**QUESTION** codez la méthode ***predict*** de la classe booting\n",
    "\n",
    "testons la sur l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "id": "57a6ecae",
   "metadata": {},
   "source": [
    "predictions_boost=ada.predict(critiques_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(classes_train,predictions_boost,digits=5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eacd656e",
   "metadata": {},
   "source": [
    "sur le test"
   ]
  },
  {
   "cell_type": "code",
   "id": "3c3cecfb",
   "metadata": {},
   "source": [
    "predictions_boost_test=ada.predict(critiques_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(classes_test,predictions_boost_test,digits=5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "04778776",
   "metadata": {},
   "source": [
    "### 6) voir l'effacité de la combinaison\n",
    "\n",
    "**QUESTION**\n",
    "- obtenez le résultats en combinant 1, puis 2, puis n classifieurs faibles\n",
    "- tracer la courbe, du f1_score en fonction du nombre de classifieurs faibles sur le train et le test"
   ]
  },
  {
   "cell_type": "code",
   "id": "a2e763d7",
   "metadata": {},
   "source": [
    "predictions_boost_test_list = [ada.predict(critiques_test,n=i) for i in range(1,51)]\n",
    "predictions_boost_train_list = [ada.predict(critiques_train,n=i) for i in range(1,51)]\n",
    "\n",
    "reports_test = [classification_report(classes_test,predictions_boost_test_list[i],output_dict=True) for i in range(len(predictions_boost_test_list))]\n",
    "reports_train = [classification_report(classes_train,predictions_boost_train_list[i],output_dict=True) for i in range(len(predictions_boost_train_list))]\n",
    "\n",
    "f1_scores_test = [report['weighted avg']['f1-score'] for report in reports_test]\n",
    "f1_scores_train = [report['weighted avg']['f1-score'] for report in reports_train]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5136730",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(f1_scores_test,label='test')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed951104",
   "metadata": {},
   "source": [
    "plt.plot(f1_scores_train,label='train')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
