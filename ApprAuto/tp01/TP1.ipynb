{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic use of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pre-formated data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# load the IRIS dataset\n",
    "from sklearn.datasets import load_iris\n",
    "irisData=load_iris()\n",
    "# get info on the dataset\n",
    "#print(irisData.data)\n",
    "print(irisData.target)\n",
    "print(irisData.target_names)\n",
    "print(irisData.feature_names)\n",
    "#print(irisData.DESCR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1:** What type of machine learning problem is that?\n",
    "\n",
    "It's a multi-class classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2:** How many features are there? What kind of features?\n",
    "\n",
    "There are 4 features. They are all numerical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt # replace the name \"pyplot\" by \"plt\" \n",
    "X=irisData.data\n",
    "y=irisData.target\n",
    "xi=0\n",
    "yi=1\n",
    "\n",
    "colors=[\"red\",\"green\",\"blue\"] # associate a color to each class label\n",
    "for num_label in range(3): # for each label\n",
    "        plt.scatter(X[y==num_label][:, xi],X[y==num_label][:,yi],color=colors[num_label],label=irisData.target_names[num_label])\n",
    "plt.legend()\n",
    "plt.xlabel(irisData.feature_names[xi]) \n",
    "plt.ylabel(irisData.feature_names[yi])\n",
    "plt.title(\"Iris Data - size of the sepals only\") \n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3:** From the previous visualisation, what can you predict about the difficulty of this dataset?\n",
    "\n",
    "Separating the blue and green classes will be hard because they are mixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying with kNN"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn import neighbors\n",
    "nb_neighb = 15\n",
    "clf = neighbors.KNeighborsClassifier(nb_neighb) # to know more about the parameters, type help(neighbors.KNeighborsClassifier)\n",
    "\n",
    "clf.fit(X, y) # training\n",
    "print('accuracy on X is',clf.score(X,y))\n",
    "\n",
    "# to predict on a specific example\n",
    "print('class predicted is',clf.predict([[ 5.4, 3.2, 1.6, 0.4]]))\n",
    "print('proba of each class is',clf.predict_proba([[ 5.4, 3.2, 1.6, 0.4]]))\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "print('misclassified training examples are:',X[y_pred!=y])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4:** What kind of problem do you see with the evaluation?\n",
    "\n",
    "The model is evaluated on the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if we want a test set and a training set, we can split the data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, y_train = X[0:100], y[0:100] # 100 examples for training\n",
    "X_test, y_test = X[100:], y[100:] # rest for testing"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5:** Explain why it is a really bad idea to split this iris dataset as we've done.\n",
    "\n",
    "The classes are not equally distributed in the training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is a much better way to split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "import random # to generate random numbers\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=random.seed()) # if needed: help(train_test_split)\n",
    "print('size of train / test = ',len(X_train), len(X_test))\n",
    "print('nb of training data with class 0/1/2 =', len(X_train[y_train==0]) ,len(X_train[y_train==1]), len(X_train[y_train==2]))\n",
    "\n",
    "clf=clf.fit(X_train, y_train)\n",
    "y_pred =clf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix\\n',cm)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6:** What is on the diagonal of the confusion matrix?\n",
    "\n",
    "The number of good predictions for each class.\n",
    "\n",
    "**Q7:** What is the real error rate (give details)?\n",
    "\n",
    "The error rate is the number of errors divided by the total number of tests.\n",
    "Here it's 3/45."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can prefer cross-fold validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nb_folds = 10\n",
    "kf=KFold(n_splits=nb_folds,shuffle=True)\n",
    "score=0\n",
    "for training_ind,test_ind in kf.split(X):\n",
    "    #print(\"training index: \",training_ind,\"\\ntest index:\",test_ind,'\\n') \n",
    "    X_train=X[training_ind]\n",
    "    y_train=y[training_ind]\n",
    "    clf.fit(X_train, y_train)\n",
    "    X_test=X[test_ind]\n",
    "    y_test=y[test_ind]\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = score + accuracy_score(y_pred,y_test)\n",
    "print('average accuracy:',score/nb_folds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or as a one-liner:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "t_scores = cross_val_score(clf, X, y, cv=10)\n",
    "print(t_scores.mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# we will use another dataset (a CSV file). Pandas helps us to read this type of file.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = 'heart.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
    "\n",
    "features = X.columns\n",
    "classes = ['Not heart disease','heart disease']\n",
    "\n",
    "print (features)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "from graphviz import Source\n",
    "\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth=20,criterion='entropy')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None,\n",
    "                                    feature_names=features,\n",
    "                                    class_names=classes,\n",
    "                                    filled=True, rounded=True))\n",
    "graph"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Graphviz is not working with your setup, look at http://people.irisa.fr/Vincent.Claveau/cours/fd/TP1.html\n",
    "\n",
    "**Q8:** Explain each line displayed in the nodes/leaves of the tree.\n",
    "\n",
    "The first line is the criteria used to split the classes.\n",
    "The second line is the entropy of the node.\n",
    "The third line is the number of samples in the node.\n",
    "The fourth line is the number of samples in each class (here there are two classes).\n",
    "The fifth line is the class that is predicted if the node is a leaf.\n",
    "    \n",
    "**Q9:** What is the name of this decision tree according to the course?\n",
    "\n",
    "It's a binary classification tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another nice viz of the decision tree. (The dtreeviz package is available in github. It can be installed with 'pip install dtreeviz'. It requires graphviz to be installed.)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from dtreeviz import * # remember to load the package\n",
    "\n",
    "graph = model(clf, X_train, y_train,\n",
    "                target_name=\"target\",\n",
    "                feature_names=features,\n",
    "                class_names=classes\n",
    "                )\n",
    "\n",
    "graph.view()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10:** Explain what are the histograms displayed.\n",
    "\n",
    "The histograms show the distribution of the classes in a node on the node's criteria. \n",
    "The color of the bars is the class that is predicted if the node is a leaf. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11** From the sklearn manual, explain what effectmax_depth or min_samples_split will have on the decision tree. If time permits, show the effects experimentally.\n",
    "\n",
    "- **max_depth**: The maximum depth of the tree.\n",
    "- **min_samples_split**: The minimum number of samples required to split an internal node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning Tmax\n",
    "\n",
    "(from https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html)\n",
    "\n",
    "Here, we use a critrion called \"Cost Complexity\". Cost complexity pruning is all about finding the right parameter for alpha.We will get the alpha values for this tree and will check the accuracy with the pruned trees."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "print(ccp_alphas)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# For each alpha we will append our model to a list\n",
    "t_clf = []\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    clf = tree.DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    t_clf.append(clf)\n",
    "    \n",
    "# we remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node.\n",
    "t_clf = t_clf[:-1]\n",
    "ccp_alphas = ccp_alphas[:-1]\n",
    "node_counts = [clf.tree_.node_count for clf in t_clf]\n",
    "depth = [clf.tree_.max_depth for clf in t_clf]\n",
    "plt.scatter(ccp_alphas,node_counts)\n",
    "plt.scatter(ccp_alphas,depth)\n",
    "plt.plot(ccp_alphas,node_counts,label='no of nodes',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,depth,label='depth',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Tree complexity vs alpha')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# accuracy versus alpha\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "for c in t_clf:\n",
    "    y_train_pred = c.predict(X_train)\n",
    "    y_val_pred = c.predict(X_val)\n",
    "    train_acc.append(accuracy_score(y_train_pred,y_train))\n",
    "    val_acc.append(accuracy_score(y_val_pred,y_val))\n",
    "\n",
    "plt.scatter(ccp_alphas,train_acc)\n",
    "plt.scatter(ccp_alphas,val_acc)\n",
    "plt.plot(ccp_alphas,train_acc,label='train_accuracy',drawstyle=\"steps-post\")\n",
    "plt.plot(ccp_alphas,val_acc,label='val_accuracy',drawstyle=\"steps-post\")\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs alpha')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12:** from the graph above, what is the best value for alpha. Replace it in the first line below ."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "best_alpha = 0.02\n",
    "clf_ = tree.DecisionTreeClassifier(random_state=0,ccp_alpha=best_alpha)\n",
    "clf_.fit(X_train,y_train)\n",
    "y_train_pred = clf_.predict(X_train)\n",
    "y_val_pred = clf_.predict(X_val)\n",
    "\n",
    "print('Train score', accuracy_score(y_train_pred,y_train))\n",
    "print(confusion_matrix(y_train_pred,y_train))\n",
    "\n",
    "print('Validation score', accuracy_score(y_val_pred,y_val))\n",
    "print(confusion_matrix(y_val_pred,y_val))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit learn implements several variants of Bayesian learning, based on different assumptions about the data https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "data = 'weather.nominal.csv'\n",
    "df = pd.read_csv(data)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13:** Let us consider the weather_nominal dataset. What is the type of each feature?\n",
    "\n",
    "The features are all nominal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train = df.drop(columns=['play'])\n",
    "y_train = df['play']\n",
    "\n",
    "features = X_train.columns\n",
    "classes = ['no play','play']\n",
    "\n",
    "# we must convert the nominal features into integers\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "X_train = enc.fit_transform(X_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.naive_bayes import CategoricalNB\n",
    "\n",
    "clf = CategoricalNB().fit(X_train,y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "print('Train score',accuracy_score(y_train_pred,y_train))\n",
    "print(confusion_matrix(y_train_pred,y_train))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14:** Explain what is displayed by the two following code lines and link that with what you've seen during the course. Do these figures correspond to what you get when doing it by yourself (explain)?\n",
    "\n",
    "The first line displays the prior probabilities *(probabilités à priori)* of the classes. \n",
    "The second line displays the conditional probabilities of the features given the classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(clf.class_log_prior_)\n",
    "print(clf.feature_log_prob_)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15:** Let's consider the weather.csv dataset now. Explain what is the difference with the previous one. \n",
    "\n",
    "Some features are numerical : the temperature and the humidity.\n",
    "\n",
    "**Q16:** Compute 'by hand' the a posteriori proba of each class for the following data sample P(play=0|x=\\['sunny',73,81,'TRUE'\\]) and  P(play=1|x=\\['sunny',73,81,'TRUE'\\]) :"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "file = 'weather.csv'\n",
    "df = pd.read_csv(file)\n",
    "\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "X_train = df.drop(columns=['play'])\n",
    "y_train = df['play']\n",
    "\n",
    "X_test = [ ['sunny',73,81,'TRUE'] ]\n",
    "\n",
    "features = X_train.columns\n",
    "classes = ['no play','play']\n",
    "\n",
    "# TODO : Fix the errors\n",
    "enc = OrdinalEncoder()\n",
    "X_train[['outlook','windy']] = enc.fit_transform(X_train[['outlook','windy']])\n",
    "\n",
    "clf = CategoricalNB().fit(X_train,y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "\n",
    "print('Train score',accuracy_score(y_train_pred,y_train))\n",
    "print(confusion_matrix(y_train_pred,y_train))\n",
    "\n",
    "\n",
    "\n",
    "print('proba of each class is',clf.predict_proba(enc.transform(X_test)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
